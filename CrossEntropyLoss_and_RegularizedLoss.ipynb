{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hands-On Lab: Understanding Cross-Entropy Loss and Regularized Loss in Neural Networks\n",
    "Estimated time needed: 60 minutes\n",
    "\n",
    "Overview:\n",
    "In this hands-on lab, you will explore essential loss functions used in neural networks for binary classification problems. Specifically, you will calculate the Cross-Entropy Loss, integrate L2 Regularization, and analyze how modifying predictions affects loss and model calibration.\n",
    "\n",
    "Loss functions play a vital role in guiding neural networks to learn optimal parameters. Regularization helps prevent overfitting, ensuring better generalization. Understanding these concepts is crucial for mastering neural network training.\n",
    "\n",
    "Objectives\n",
    "By the end of this lab, learners will be able to:\n",
    "\n",
    "Compute the Cross-Entropy Loss for a binary classification task.\n",
    "Integrate L2 Regularization with a loss function to compute the Regularized Loss.\n",
    "Modify predictions to reduce loss and discuss the implications for calibration.\n",
    "Dataset Description\n",
    "In this lab, there is no external dataset. We will use a synthetic example:\n",
    "\n",
    "True Labels (y_true): [1, 0, 1, 1, 0] Ground-truth binary labels.\n",
    "Predicted Probabilities (y_pred): [0.9, 0.2, 0.8, 0.7, 0.1] Model-predicted probabilities for the positive class.\n",
    "Assignment Tasks\n",
    "Step 1: Install and Import Necessary Libraries¬∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all necessary libraries\n",
    "pip install numpy\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Compute Cross-Entropy Loss\n",
    "Cross-Entropy Loss measures the difference between the true labels and the predicted probabilities. For binary classification, it is calculated as:\n",
    "\n",
    "Cross-Entropy Loss=‚àí1ùëÅ‚àëùëñ=1ùëÅ(ùë¶ùëñlog(ùë¶ÃÇ ùëñ)+(1‚àíùë¶ùëñ)log(1‚àíùë¶ÃÇ ùëñ))\n",
    " \n",
    "\n",
    "Here, ( y_i ) is the true label, and  ùë¶ÃÇ ùëñ\n",
    "  is the predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given data\n",
    "y_true = np.array([1, 0, 1, 1, 0])\n",
    "y_pred = np.array([0.9, 0.2, 0.8, 0.7, 0.1])\n",
    "\n",
    "# Cross-Entropy Loss function\n",
    "# Write your code here!\n",
    "def compute_cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15  # To avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predictions\n",
    "    #Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. \n",
    "    # For example: If `y_pred = 0`, it becomes `1e-15`. If `y_pred = 1`, it becomes `1 - 1e-15`\n",
    "    #This avoids errors like `log(0)` in the next line.\n",
    "    cross_entropy_loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return cross_entropy_loss\n",
    "\n",
    "\n",
    "# Calculate loss\n",
    "cross_entropy_loss = compute_cross_entropy_loss(y_true, y_pred)\n",
    "print(f\"Cross-Entropy Loss: {cross_entropy_loss:.4f}\")\n",
    "\n",
    "#Output: Cross-Entropy Loss: 0.2027"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Compute Regularized Loss (L2 Regularization)\n",
    "To prevent overfitting, we add a regularization term to the loss function. Using L2 Regularization:\n",
    "\n",
    "Regularized Loss=Cross-Entropy Loss+ùúÜ‚ãÖ12ùëÅ‚àëùëñ=1ùëÅùë§2ùëñ\n",
    " \n",
    "\n",
    "Here, Œª is the regularization strength, and ( w_i ) are model weights.\n",
    "\n",
    "For simplicity, assume weights = [0.5, -0.3, 0.8, -0.2, 0.1] and Œª = 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized Loss function\n",
    "# Write your code here!\n",
    "def compute_regularized_loss(y_true, y_pred, weights, lambda_val):\n",
    "    cross_entropy_loss = compute_cross_entropy_loss(y_true, y_pred)\n",
    "    l2_term = lambda_val * (np.sum(np.square(weights)) / (2 * len(weights))) #len(weights) gives 'N'\n",
    "    regularized_loss = cross_entropy_loss + l2_term\n",
    "    return regularized_loss\n",
    "\n",
    "\n",
    "# Given weights and lambda\n",
    "weights = np.array([0.5, -0.3, 0.8, -0.2, 0.1])\n",
    "lambda_val = 0.5\n",
    "\n",
    "# Calculate regularized loss\n",
    "regularized_loss = compute_regularized_loss(y_true, y_pred, weights, lambda_val)\n",
    "print(f\"Regularized Loss: {regularized_loss:.4f}\")\n",
    "\n",
    "#Output: Regularized Loss: 0.2542"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Modify Predictions and Discuss Calibration\n",
    "Modify y_pred to reduce the Cross-Entropy Loss. Observe how this impacts calibration the alignment between predicted probabilities and observed outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified predictions \n",
    "y_pred_modified = np.array([1.0, 0.0, 1.0, 1.0, 0.0])\n",
    "cross_entropy_loss_modified = compute_cross_entropy_loss(y_true, y_pred_modified)\n",
    "\n",
    "# Print the value of the modified cross-entropy loss, rounded to 4 decimal places.\n",
    "# Write your code here!\n",
    "print(f\"Modified Cross-Entropy Loss: {cross_entropy_loss_modified:.4f}\")\n",
    "\n",
    "# Before Modification:\n",
    "# y_pred = np.array([0.9, 0.2, 0.8, 0.7, 0.1])  # Example calibrated probabilities\n",
    "# These predictions reflect reasonable confidence levels.\n",
    "# They are calibrated, meaning:\n",
    "# If the model says 0.8, it expects to be right ~80% of the time.\n",
    "# However, even if correct, the log loss is not zero because:\n",
    "# Loss = -log(0.8)‚â†0\n",
    "# So, the cross-entropy loss is higher (0.2027), even though the predictions are good and realistic.\n",
    "\n",
    "\n",
    "# After Modification:\n",
    "# y_pred_modified = np.array([1.0, 0.0, 1.0, 1.0, 0.0])\n",
    "# These predictions are extremely confident ‚Äî the model is saying ‚ÄúI'm 100% sure.‚Äù\n",
    "# If these are all correct, the cross-entropy loss becomes: ‚àílog(1.0)=0\n",
    "# So the loss is minimized (even zero; 0.0000).\n",
    "#‚ùóBut this is not calibrated: in real-world models, predicting 0.0 or 1.0 is often overconfident and risky.\n",
    "# If just one of these was wrong, the loss would be massive due to log(0 + Œµ).\n",
    "\n",
    "#Output: Modified Cross-Entropy Loss: 0.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Before modification: Predicted probabilities are calibrated but may result in higher loss.\n",
    "After modification: Loss decreases, but predictions are no longer calibrated, making them overconfident.\n",
    "Key Takeaways\n",
    "Cross-Entropy Loss quantifies the accuracy of predicted probabilities.\n",
    "L2 Regularization penalizes large weights, helping prevent overfitting.\n",
    "Overconfident predictions can reduce loss but may harm calibration, affecting real-world model performance.\n",
    "Summary:\n",
    "In this lab, you:\n",
    "\n",
    "Computed Cross-Entropy Loss for binary classification.\n",
    "Integrated L2 Regularization to calculate the Regularized Loss.\n",
    "Modified predictions to explore the trade-offs between loss reduction and calibration.\n",
    "Understanding and balancing these metrics is essential for building robust and interpretable neural networks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
