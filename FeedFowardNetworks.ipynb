{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hands-On Lab: Implementing a Simple Feedforward Neural Network on the Iris Dataset\n",
    "Estimated time needed: 60 minutes\n",
    "\n",
    "Overview:\n",
    "In this hands-on lab, you will build a simple feedforward neural network to classify flowers in the Iris dataset into three categories: Setosa, Versicolor, and Virginica. The lab involves using Python libraries such as Keras, TensorFlow, and Scikit-learn for data preprocessing, model creation, training, and evaluation.\n",
    "\n",
    "You will explore how different optimization algorithms SGD, Momentum, and Adam impact the convergence rate and overall model performance. This lab is designed to provide practical experience with deep learning concepts and expose learners to essential machine learning workflows.\n",
    "\n",
    "Objectives\n",
    "By the end of this lab, learners will be able to:\n",
    "\n",
    "Load and preprocess the Iris dataset, including feature scaling and target one-hot encoding.\n",
    "Implement a feedforward neural network using Keras or TensorFlow.\n",
    "Train and evaluate the neural network using different optimization algorithms:\n",
    "Stochastic Gradient Descent (SGD)\n",
    "SGD with Momentum\n",
    "Adam Optimizer\n",
    "Visualize and compare the performance of each optimizer using metrics like accuracy, precision, recall, and the confusion matrix.\n",
    "Understand the importance of preprocessing, network design, and hyperparameter tuning in building effective models.\n",
    "Dataset Used\n",
    "Iris Dataset:\n",
    "The Iris dataset is a widely used toy dataset for classification tasks. It contains 150 samples of iris flowers with the following characteristics:\n",
    "\n",
    "Features (4): Sepal length, Sepal width, Petal length, Petal width.\n",
    "Target Classes (3):\n",
    "Setosa (label 0)\n",
    "Versicolor (label 1)\n",
    "Virginica (label 2)\n",
    "Each sample belongs to one of these three categories. The dataset is simple and well-separated, making it ideal for experimenting with basic machine learning and neural networks.\n",
    "\n",
    "Implementation:\n",
    "Step 1: Install and Import Necessary Libraries\n",
    "Note: Please be patient while the required packages are being installed. This may take some time due to the multiple dependencies, but it should complete shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all necessary libraries\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Load and Explore the Dataset\n",
    "Explanation: This code loads the Iris dataset and creates a DataFrame for easier analysis. The pairplot helps visualize relationships between features and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n",
    "                    columns=iris['feature_names'] + ['target'])\n",
    "# np.c_ is a NumPy shortcut used for column-wise concatenation of arrays.\n",
    "# np.c_[iris['data'], iris['target']]:\n",
    "# combines the feature matrix iris['data'] (shape 150Ã—4) with the target vector iris['target'] (shape 150Ã—1) \n",
    "# into a single 150Ã—5 matrix.\n",
    "# This line creates a Pandas DataFrame called 'data' by combining the features and the target into one table.\n",
    "# columns=iris['feature_names'] + ['target'] --> This sets the column names\n",
    "# iris['feature_names'] is:\n",
    "# ['sepal length (cm)', 'sepal width (cm)',\n",
    "# 'petal length (cm)', 'petal width (cm)']\n",
    "# Add ['target'] to get 5 total column names.\n",
    "\n",
    "\n",
    "\n",
    "# Display dataset information\n",
    "print(data.info())  # Shows data types and null values\n",
    "print(data.head())  # Displays the first 5 rows of the dataset\n",
    "\n",
    "# Visualize pairplot\n",
    "sns.pairplot(data, hue='target', diag_kind='kde', markers=[\"o\", \"s\", \"D\"])\n",
    "plt.show()  # Pairwise plots of features with species color-coded\n",
    "# kde stands for Kernel Density Estimation.\n",
    "# In sns.pairplot(), diag_kind='kde' means that the diagonal plots \n",
    "# (i.e., distributions of individual features) should be shown as smooth curves instead of histograms.\n",
    "# 'kde': smooth density plots\n",
    "# 'hist': regular histograms\n",
    "# markers=[\"o\", \"s\", \"D\"]\n",
    "# This controls marker styles for different classes in the scatterplots of the pairplot.\n",
    "# \"o\" = circle marker (for one class)\n",
    "# \"s\" = square marker\n",
    "# \"D\" = diamond marker\n",
    "# So in our pairplot, each class (Setosa, Versicolor, Virginica) will get a different marker symbol in the scatterplots.\n",
    "# hue = 'target' means the 3 species (Setosa, Versicolor, Virginica) will each be assigned a different color in the plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Preprocess the Data\n",
    "Explanation: The data is split into training and testing sets. The target is converted to one-hot encoding (e.g., [1, 0, 0] for Setosa). Features are standardized for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Features and target\n",
    "X = iris['data']  # Feature matrix (sepal and petal measurements)\n",
    "y = iris['target'].reshape(-1, 1)  # Target values reshaped into column format\n",
    "# Why reshape?\n",
    "# OneHotEncoder expects a 2D input. A 1D array would cause an error.\n",
    "\n",
    "# One-hot encode target\n",
    "# What is One-Hot Encoding (aka One-Hot Format)?\n",
    "# One-hot encoding is a way to represent categorical (non-numeric) data as numbers, especially for use in ML models.\n",
    "# Turn each category/class into a binary vector where:\n",
    "# Only one element is 1 (hot),\n",
    "# the rest are 0 (cold).\n",
    "# Label\tClass\n",
    "# 0\tSetosa\n",
    "# 1\tVersicolor\n",
    "# 2\tVirginica\n",
    "# Using one-hot encoding, these become:\n",
    "# Class\tOne-Hot Vector\n",
    "# Setosa\t[1, 0, 0]\n",
    "# Versicolor\t[0, 1, 0]\n",
    "# Virginica\t[0, 0, 1]\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Use 'sparse_output' instead of 'sparse'\n",
    "y_encoded = encoder.fit_transform(y)  # Convert target to one-hot encoding\n",
    "# This line^ tells the encoder to:\n",
    "# Learn the possible categories (0, 1, 2)\n",
    "# Transform each label into a one-hot vector\n",
    "\n",
    "# Why Use One-Hot Encoding?\n",
    "# Machine learning models (especially neural networks) canâ€™t understand strings or raw class numbers directly:\n",
    "# If we use 0, 1, 2 directly, the model might think class 2 is greater than class 0, which isnâ€™t true.\n",
    "# One-hot removes any implied order or ranking.\n",
    "\n",
    "# encoder = OneHotEncoder(sparse_output=False) --> Creates an instance of the encoder.\n",
    "# sparse_output=False means the output will be a regular NumPy array, \n",
    "# not a sparse matrix (which saves memory but is harder to work with).\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "# X: The feature matrix (flower measurements: sepal length, petal width, etc.)\n",
    "# y_encoded: The one-hot encoded target labels (e.g., [1,0,0] for Setosa)\n",
    "# test_size=0.2: 20% of the data goes to the test set, 80% to training\n",
    "# random_state=42: Controls the random shuffling of the data before splitting â€” ensures reproducibility\n",
    "# X_train: Features for training (80%)\n",
    "# X_test: Features for testing (20%)\n",
    "# y_train: One-hot targets for training\n",
    "# y_test: One-hot targets for testing\n",
    "\n",
    "# Assuming there's 150 samples:\n",
    "# X_train: (120, 4) --> 80% of the samples (features) for training\n",
    "# X_test: (30, 4) --> 20% of the samples (features) for testing\n",
    "# y_train: (120, 3) --> 80% of the labels (one-hot encoded)\n",
    "# y_test: (30, 3) --> 20% of the labels (one-hot encoded)\n",
    "\n",
    "# For X_train and X_test, '4' comes from the 4 features in the Iris dataset:\n",
    "# The Iris dataset has 4 feature columns: Sepal length, Sepal width, Petal length, Petal width\n",
    "\n",
    "# For y_train and y_test, '3' comes from the number of classes/species:\n",
    "# The Iris dataset has 3 unique flower classes: Setosa (class 0), Versicolor (class 1), Virginica (class 2)\n",
    "# When we apply one-hot encoding, each label is converted into a vector of length 3\n",
    "\n",
    "# Normalize features\n",
    "# We want to perform a standard scaling on the petal features. \n",
    "# Transforms the petal features measurements to a range of numbers with mean = 0 and standard deviation = 1.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Standardize features for stability\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# scaler = StandardScaler() --> Creates a StandardScaler object from sklearn.preprocessing.\n",
    "# This object, 'scaler', is ready to standardize the features â€” meaning:\n",
    "# StandardizedÂ value=(ð‘¥âˆ’mean)/standardÂ deviation\n",
    "\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# What it does:\n",
    "# Fits the scaler only on X_train:\n",
    "# It computes the mean and standard deviation for each feature in the training data.\n",
    "# Then it transforms the training data:\n",
    "# Subtracts the mean and divides by the std dev for each feature â†’ all features now have mean 0 and std dev 1.\n",
    "# This is called z-score normalization.\n",
    "# Important: We fit only on X_train to avoid data leakage â€” we donâ€™t want the test data to influence model training.\n",
    "\n",
    "# X_test = scaler.transform(X_test)\n",
    "# What it does:\n",
    "# Uses the same mean and std (from training data) to scale the test data.\n",
    "# Ensures that both train and test features are on the same scale, based on training stats.\n",
    "# Many ML models (like logistic regression, neural networks) work much better when input features are on the same scale.\n",
    "# Note: Never call fit_transform() on X_test â€” that would be cheating by using test info during training.\n",
    "\n",
    "# The fit(data) method is used to compute the mean and std dev for a given feature to be used further for scaling.\n",
    "# The transform(data) method is used to perform scaling using mean and std dev calculated using the .fit() method.\n",
    "# The fit_transform() method does both fits and transform.\n",
    "\n",
    "# The fit() function computes the formulation to transform the column based on Standard scaling,\n",
    "# but doesnâ€™t apply the actual transformation. \n",
    "# The computation is stored as a fit object. The fit method doesnâ€™t return anything.\n",
    "\n",
    "# The transform() method takes advantage of the fit object in the fit() method and \n",
    "# applies the actual transformation onto the column. \n",
    "# So, fit() and transform() is a two-step process that completes the transformation in the second step. \n",
    "# Here, unlike the fit() method, the transform() method returns the actually transformed array.\n",
    "\n",
    "\n",
    "# The fit_transform() Method:\n",
    "# fit() and transform() is a two-step process, which can be brought down to a one-shot process using the fit_transform() method\n",
    "# When the fit_transform() method is used, we can compute and apply the transformation in a single step.\n",
    "\n",
    "\n",
    "# scaler.fit_transform(X_train)\n",
    "# - Fits on X_train: computes mean and standard deviation of each feature\n",
    "# - Then transforms X_train: subtracts mean and divides by std (standardization)\n",
    "\n",
    "# scaler.transform(X_test)\n",
    "# - Does NOT fit again (no new statistics calculated)\n",
    "# - Uses the same mean and std from X_train to transform X_test\n",
    "# - Ensures test data is treated consistently (no data leakage)\n",
    "\n",
    "# Why the distinction matters:\n",
    "# fit()/fit_transform() should only be used on training data\n",
    "# transform() should be used on test or new data, using the ALREADY LEARNED parameters\n",
    "\n",
    "# so what even stores these 'already learned parameters' from X_train to be reused in X_test?:\n",
    "# The scaler object stores the learned statistics (like mean and standard deviation for each feature) inside itself.\n",
    "# Internally Stored:\n",
    "# After .fit_transform(X_train), the scaler object contains:\n",
    "# scaler.mean_: an array of means for each feature in X_train\n",
    "# scaler.scale_: the standard deviations of each feature\n",
    "# scaler.var_: the variances (if needed)\n",
    "# So when we later call:\n",
    "# X_test = scaler.transform(X_test)\n",
    "# It uses these stored values (from training data) to standardize X_test.\n",
    "# 'scaler' object remembers the statistics from the .fit_transform() step and reuses them during .transform(). \n",
    "# This ensures consistency and avoids data leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Design the Feedforward Neural Network\n",
    "Explanation: The network has two hidden layers with ReLU activation and one output layer with softmax activation to predict probabilities for the three classes, Setosa, Virginica and Versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(10, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer + 1st hidden layer\n",
    "        Dense(8, activation='relu'),  # 2nd hidden layer\n",
    "        Dense(3, activation='softmax')  # Output layer with 3 classes\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "#We're using Keras' Sequential API here, which stacks layers one after the other.\n",
    "# 1. Input Layer + First Hidden Layer:\n",
    "# Dense(10, activation='relu', input_shape=(X_train.shape[1],))\n",
    "# - A Dense (fully connected) layer with 10 neurons\n",
    "# - ReLU activation (introduces non-linearity)\n",
    "# - input_shape=(X_train.shape[1],) means the input layer expects a vector with the same number of features as our training data\n",
    "\n",
    "# 2. Second Hidden Layer:\n",
    "# Dense(8, activation='relu')\n",
    "# - Another dense layer with 8 neurons and ReLU activation\n",
    "# - Learns more abstract patterns\n",
    "\n",
    "# 3. Output Layer:\n",
    "# Dense(3, activation='softmax')\n",
    "# - Final layer with 3 neurons (because the Iris dataset has 3 classes)\n",
    "# - Softmax activation outputs probabilities for each class (they sum to 1)\n",
    "\n",
    "# Return the Model:\n",
    "# return model\n",
    "\n",
    "# Calling create_model() will give us a new neural network that looks like this:\n",
    "# Input (4 features) â†’ Dense(10, ReLU) â†’ Dense(8, ReLU) â†’ Dense(3, Softmax)\n",
    "\n",
    "# What is the '1' in X_train.shape[1]?\n",
    "# In NumPy or Pandas, .shape gives the dimensions (rows and columns) of an array or DataFrame:\n",
    "# X_train.shape: For example, might return (120, 4)\n",
    "# This means:\n",
    "# - 120 = number of training samples (rows)\n",
    "# - 4 = number of features (columns), like sepal length, sepal width, etc.\n",
    "# So, X_train.shape[1] means:\n",
    "# \"Give me the number of features (columns) in X_train\" --> index 1 is '4', the number of features\n",
    "# This is needed because the first layer of the neural network must know how many inputs it will receive, which is the num. of features\n",
    "\n",
    "# .shape returns a tuple â†’ e.g., (120, 4)\n",
    "# Index 0 is number of samples\n",
    "# Index 1 is number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Train the Model with Different Optimization Algorithms\n",
    "Explanation: The model is trained using different optimizers: SGD (basic gradient descent), Momentum (adds velocity), and Adam (adaptive learning rates). Training history is logged for each optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "# SGD: Stochastic Gradient Descent\n",
    "# Adam: Adaptive Moment Estimation\n",
    "\n",
    "# Train model with different optimizers\n",
    "optimizers = {'SGD': SGD(learning_rate=0.01), \n",
    "              'Momentum': SGD(learning_rate=0.01, momentum=0.9), \n",
    "              'Adam': Adam(learning_rate=0.01)} #dictionary of optimisers \n",
    "\n",
    "histories = {}\n",
    "\n",
    "# 'SGD': SGD(learning_rate=0.01)\n",
    "# SGD (Stochastic Gradient Descent) updates weights using small batches of data.\n",
    "#learning_rate=0.01 means the model updates weights in small steps (0.01 of the gradient value).\n",
    "# Why 0.01? Itâ€™s a commonly used starting value that balances:\n",
    "# - Learning fast enough\n",
    "# - Avoiding overshooting the optimal point\n",
    "\n",
    "# 'Momentum': SGD(learning_rate=0.01, momentum=0.9)\n",
    "# Momentum helps SGD move faster through shallow or noisy regions by adding a fraction of the previous update.\n",
    "# momentum=0.9 means: 90% of the previous stepâ€™s direction is carried into the current one.\n",
    "# Why 0.9? Empirically, values like 0.9 or 0.99 usually help models converge more quickly and avoid getting stuck.\n",
    "\n",
    "# 'Adam': Adam(learning_rate=0.01)\n",
    "# Adam combines Momentum + Adaptive Learning Rate.\n",
    "# learning_rate=0.01 is used here for comparison with SGD, but Adam usually works better with smaller rates like 0.001 or even 0.0001.\n",
    "\n",
    "# Loop through each optimizer, train the model, evaluate its performance, and store the training history for comparison.\n",
    "for opt_name, optimizer in optimizers.items():\n",
    "    print(f\"\\nTraining with {opt_name} optimizer...\")\n",
    "    model = create_model()\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])  # Compile model\n",
    "    history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), batch_size=8, verbose=0)  # Train\n",
    "    histories[opt_name] = history.history  # Store training history\n",
    "    print(f\"Final Test Accuracy with {opt_name}: {model.evaluate(X_test, y_test, verbose=0)[1]:.4f}\")\n",
    "\n",
    "# compile() is a built-in method from Keras' Sequential class\n",
    "# It prepares the model for training by setting:\n",
    "# - the optimizer (how the model learns)\n",
    "# - the loss function (how error is measured)\n",
    "# - the metrics (how performance is evaluated\n",
    "\n",
    "#categorical_crossentropy is a loss function used for multi-class classification when our target (label) is one-hot encoded.\n",
    "# Example:\n",
    "# True label (Setosa): [1, 0, 0]\n",
    "# Model prediction: [0.7, 0.2, 0.1] -> The output layer uses Softmax activation, which outputs probabilities for each class (they sum to 1)\n",
    "# The function computes how \"wrong\" the prediction is based on the difference between the predicted probabilities and the true one-hot vector.\n",
    "\n",
    "\n",
    "# 'accuracy' is also a built-in metric in Keras.\n",
    "# It tells the model to report accuracy during training and validation:\n",
    "# i.e., â€œHow often was the predicted class correct?â€\n",
    "# Other available metrics include:\n",
    "# - 'mae' (mean absolute error)\n",
    "# - 'mse' (mean squared error)\n",
    "# - 'precision', 'recall' etc. \n",
    "\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), batch_size=8, verbose=0):\n",
    "# - X_train, y_train: training data\n",
    "# - validation_data=(X_test, y_test): evaluates on test data during training\n",
    "# - epochs=50: trains for 50 full passes through the data\n",
    "# - batch_size=8: processes 8 samples at a time\n",
    "# - verbose=0: hides output to keep things clean\n",
    "\n",
    "\n",
    "# model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "# [1] gets the accuracy value (since evaluate() returns [loss, accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clarification:\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(10, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "This is where we:\n",
    "- Design the structure (number of layers and neurons)\n",
    "- Choose activation functions\n",
    "- Define the output format (e.g., softmax for multi-class classification)\n",
    "\n",
    "Think of this as choosing what to learn and how complex the model is.\n",
    "\n",
    "And...\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "This is where we:\n",
    "- Choose how to train the network (loss function, evaluation metric)\n",
    "- Specify what method/algorithm to use to adjust weights (which optimizer to use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also take note that we (as the programmer) do not need to do the math manually for the SGD, Momentum or Adam, because behind the scenes, Keras automatically:\n",
    "- Performs a forward pass to compute predictions.\n",
    "- Computes the loss.\n",
    "- Applies backpropagation to compute gradients.\n",
    "- Uses the selected optimizer (like Adam, SGD, etc.) to update weights.\n",
    "- Repeats this for each batch over multiple epochs\n",
    "\n",
    "when we define and compile your model in Keras: model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "then call: model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Visualize and Compare Convergence Rates\n",
    "Explanation: Loss and accuracy curves are plotted for each optimizer, showing how fast they converge and their final performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy for each optimizer\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "for opt_name, history in histories.items():\n",
    "    plt.plot(history['loss'], label=f'{opt_name} Loss')  # Plot loss over epochs\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "for opt_name, history in histories.items():\n",
    "    plt.plot(history['accuracy'], label=f'{opt_name} Accuracy')  # Plot accuracy over epochs\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# 1 = number of rows in the grid\n",
    "# 2 = number of columns in the grid\n",
    "# 1 = this plot is in position 1 (i.e., the left plot)\n",
    "# So this creates a 1-row, 2-column layout, and this plot appears in the left panel.\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# Same grid: 1 row Ã— 2 columns\n",
    "# Plot is now in position 2 (i.e., the right panel)\n",
    "\n",
    "# Weâ€™re making:\n",
    "# Plot 1: Training Loss (left)\n",
    "# Plot 2: Training Accuracy (right)\n",
    "# And arranging them side by side (1 row, 2 columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Evaluate the Final Model Performance\n",
    "Explanation: The best model (using Adam) is evaluated on the test set. A classification report provides precision, recall, and F1 scores, while a confusion matrix shows how well the model distinguishes between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluate the best model (e.g., Adam optimizer used for demonstration)\n",
    "best_model = create_model()\n",
    "best_model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "best_model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)  # Retrain with Adam\n",
    "y_pred = np.argmax(best_model.predict(X_test), axis=1)  # Predicted classes\n",
    "y_true = np.argmax(y_test, axis=1)  # True classes\n",
    "\n",
    "# predict() is a built-in method in Keras models.\n",
    "# It returns the modelâ€™s output (probability scores in this case) for each input in X_test.\n",
    "# Since our model ends with Dense(3, activation='softmax'), each prediction will look like:\n",
    "# [0.1, 0.7, 0.2] -> Probability of classes 0, 1, and 2\n",
    "\n",
    "# np.argmax:\n",
    "# predictions = np.array([\n",
    "#    [0.1, 0.7, 0.2],  # model thinks class 1 is most likely (since 0.7 is the max value, and that's at index 1)\n",
    "#    [0.9, 0.05, 0.05],  # class 0 is most likely becaus 0.9 is the max. value and it's at index 0\n",
    "#    [0.2, 0.2, 0.6]   # class 2 is most likley \n",
    "# ])\n",
    "# To get the predicted class for each row, use:\n",
    "# np.argmax(predictions, axis=1) âžœ array([1, 0, 2])\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, target_names=iris['target_names']))  # Detailed metrics\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris['target_names'], yticklabels=iris['target_names'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# A confusion matrix is a table that helps us evaluate the performance of a classification model \n",
    "# by showing how often predictions match the true labels.\n",
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "# confusion_matrix() from sklearn compares our true labels vs predicted labels\n",
    "\n",
    "# sns.heatmap: Draws the matrix as a colored heatmap \n",
    "# - A heatmap is a graphical representation of data where values are represented by different colors. \n",
    "# - The idea is to quickly visualize patterns, trends, or intensity of values across a grid or matrix.\n",
    "# - Typically, darker colors (or warmer colors) represent higher values, and lighter colors (or cooler colors) represent lower values.\n",
    "\n",
    "# annot=True: Show the numbers in each cell\n",
    "# - annot=True: This means annotate the heatmap with the actual numerical values in each cell of the grid.\n",
    "# - If annot=False, the cells will just be filled with colors and no numbers will appear.\n",
    "\n",
    "# fmt='d': Format numbers as integers\n",
    "\n",
    "# cmap='Blues': Use a blue color scale\n",
    "# - cmap='Blues': The cmap parameter specifies the color map or color palette to use for the heatmap.\n",
    "# - 'Blues' is a predefined color palette in Seaborn, which shades cells from light blue (low) to dark blue (high).\n",
    "\n",
    "# xticklabels / yticklabels: Label the axes with actual class names (Setosa, etc.)\n",
    "\n",
    "# High values on the diagonal = good accuracy\n",
    "# High values off-diagonal = model made mistakes\n",
    "# This helps us see exactly which classes are being confused with each other.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
